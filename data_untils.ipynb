{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read XSum Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_lines(file_path):\n",
    "    files = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            files.append(line.strip())\n",
    "    return files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11307\n"
     ]
    }
   ],
   "source": [
    "document_path = '/home/ml/cadencao/XSum/fairseq_files/val.source'\n",
    "target_path = '/home/ml/cadencao/XSum/fairseq_files/val.target'\n",
    "xsum_source = read_lines(document_path)\n",
    "xsum_target = read_lines(target_path)\n",
    "print(len(xsum_source))\n",
    "assert len(xsum_source) == len(xsum_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11307\n"
     ]
    }
   ],
   "source": [
    "document_bpe_path = '/home/ml/cadencao/XSum/fairseq_files/val.bpe.source'\n",
    "target_bpe_path = '/home/ml/cadencao/XSum/fairseq_files/val.bpe.target'\n",
    "xsum_bpe_source = read_lines(document_bpe_path)\n",
    "xsum_bpe_target = read_lines(target_bpe_path)\n",
    "print(len(xsum_bpe_source))\n",
    "assert len(xsum_bpe_source) == len(xsum_bpe_target) == len(xsum_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load BART"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fairseq.models.bart import BARTModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "bart = BARTModel.from_pretrained('/home/ml/cadencao/Downloads/BART_models/bart.large.xsum',\n",
    "                                 checkpoint_file='model.pt',\n",
    "                                 data_name_or_path='/home/ml/cadencao/Downloads/BART_models/bart.large.xsum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- activate evaluation mode\n"
     ]
    }
   ],
   "source": [
    "bart.cuda()\n",
    "bart.eval()\n",
    "bart.half()\n",
    "print('- activate evaluation mode')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "encode_func = bart.encode\n",
    "decode_func = bart.decode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<fairseq.tasks.translation.TranslationTask at 0x7effadefa8d0>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bart.task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<fairseq.data.dictionary.Dictionary at 0x7effadefe090>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bart.task.src_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build Input to BART"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fairseq.tasks.translation import TranslationTask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Atletico Madrid goalkeeper David de Gea has said he would not feel the pressure of a high transfer fee if he were to join Manchester United.'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode_func(torch.LongTensor([ 3750, 12533,  3622,  7551,   871,   263,  4177,   102,    34,    26,\n",
    "           37,    74,    45,   619,     5,  1164,     9,    10,   239,  2937,\n",
    "         4029,   114,    37,    58,     7,  1962,  2361,   315,     4,     2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Striker Robert Vittek is the headline absentee from the provisional 27-man squad Slovakia coach Jan Kozak has named for Euro 2016.'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xsum_target[33]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([    0,  3750, 12533,  3622,  7551,   871,   263,  4177,   102,    34,\n",
       "           26,    37,    74,    45,   619,     5,  1164,     9,    10,   239,\n",
       "         2937,  4029,   114,    37,    58,     7,  1962,  2361,   315,     4,\n",
       "            2])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encode_func('Atletico Madrid goalkeeper David de Gea has said he would not feel the pressure of a high transfer fee if he were to join Manchester United.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/home/ml/cadencao/XSum/test_files/xsum-bin'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[source] dictionary: 50264 types\n",
      "[target] dictionary: 50264 types\n"
     ]
    }
   ],
   "source": [
    "src_dict = TranslationTask.load_dictionary(os.path.join(path, 'dict.{}.txt'.format('source')))\n",
    "tgt_dict = TranslationTask.load_dictionary(os.path.join(path, 'dict.{}.txt'.format('target')))\n",
    "assert src_dict.pad() == tgt_dict.pad()\n",
    "assert src_dict.eos() == tgt_dict.eos()\n",
    "assert src_dict.unk() == tgt_dict.unk()\n",
    "print('[{}] dictionary: {} types'.format('source', len(src_dict)))\n",
    "print('[{}] dictionary: {} types'.format('target', len(tgt_dict)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<fairseq.data.dictionary.Dictionary at 0x7f8b181aad10>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bpe_to_ids(fairseq_dict, sentence, addl_sentence=None, max_positions=1024, no_bos=True):\n",
    "    \"\"\"Convert bpe ids to model input ids.\n",
    "\n",
    "    Args:\n",
    "        fairseq_dict (fairseq.data.dictionary.Dictionary): fairseq dictionary.\n",
    "        sentence (str): bpe encoded sentence.\n",
    "        addl_sentence (str): bpe encoded sentence.\n",
    "        max_positions (int): max sentence length.\n",
    "        no_bos (bool): whether append bos token.\n",
    "\n",
    "    \"\"\"\n",
    "    extra_tokens = 2\n",
    "    bos, eos = '<s> ', ' </s>'\n",
    "\n",
    "    if no_bos:\n",
    "        bos = ''\n",
    "        extra_tokens = 1\n",
    "\n",
    "    if addl_sentence:\n",
    "        tokens = sentence + eos + ' ' + addl_sentence\n",
    "    else:\n",
    "        tokens = sentence\n",
    "\n",
    "    if len(tokens.split(' ')) > max_positions - extra_tokens:\n",
    "        tokens = ' '.join(tokens.split(' ')[:max_positions - extra_tokens])\n",
    "    bpe_sentence = bos + tokens + eos\n",
    "\n",
    "    tokens = fairseq_dict.encode_line(bpe_sentence, append_eos=False)\n",
    "    return tokens.long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'12510 1966 3701 4881 4409 423 587 1043 6717 286 34759 262 20858 422 262 12983 286 734 12353 19105 257 3249 546 1693 6630 13'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xsum_bpe_target[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([15622,   320,  1754,  1470,  1321,    33,    57,   303,  2181,     9,\n",
      "        28304,     5, 15331,    31,     5,  7314,     9,    80,  4585,  9886,\n",
      "           10,   529,    59,   633,  2599,     4,     2])\n"
     ]
    }
   ],
   "source": [
    "ids = bpe_to_ids(src_dict, xsum_bpe_target[0])\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Three former Air France employees have been found guilty of ripping the shirts from the backs of two executives fleeing a meeting about job cuts.'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode_func(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fairseq.data.language_pair_dataset import collate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = []\n",
    "for i in range(3):\n",
    "    samples.append({\n",
    "        'id': i,\n",
    "        'source': bpe_to_ids(src_dict, xsum_bpe_source[i]),\n",
    "        'target': bpe_to_ids(src_dict, xsum_bpe_target[i])\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "pad_idx, eos_idx = 1, 2\n",
    "batch = collate(samples, pad_idx, eos_idx, left_pad_source=True, left_pad_target=False, input_feeding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': tensor([2, 0, 1]),\n",
       " 'nsentences': 3,\n",
       " 'ntokens': 65,\n",
       " 'net_input': {'src_tokens': tensor([[ 100,   33,   57,  ...,  502,    4,    2],\n",
       "          [   1,    1,    1,  ..., 1427,    4,    2],\n",
       "          [   1,    1,    1,  ...,  443,    4,    2]]),\n",
       "  'src_lengths': tensor([668, 376, 165]),\n",
       "  'prev_output_tokens': tensor([[    2, 29042,  1252,    11,  5295,    28,   357,   160,    11,    50,\n",
       "              66,     9,     5,   796,  1332,   116,     1,     1,     1,     1,\n",
       "               1,     1,     1,     1,     1,     1,     1],\n",
       "          [    2, 15622,   320,  1754,  1470,  1321,    33,    57,   303,  2181,\n",
       "               9, 28304,     5, 15331,    31,     5,  7314,     9,    80,  4585,\n",
       "            9886,    10,   529,    59,   633,  2599,     4],\n",
       "          [    2,  9497,    33,   703, 14363,  3156,    25,   233,     9,  4941,\n",
       "              88,    41,  2080,   751,    41,  1586,  7450,  2681,  2003,    94,\n",
       "              76,     4,     1,     1,     1,     1,     1]])},\n",
       " 'target': tensor([[29042,  1252,    11,  5295,    28,   357,   160,    11,    50,    66,\n",
       "              9,     5,   796,  1332,   116,     2,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1],\n",
       "         [15622,   320,  1754,  1470,  1321,    33,    57,   303,  2181,     9,\n",
       "          28304,     5, 15331,    31,     5,  7314,     9,    80,  4585,  9886,\n",
       "             10,   529,    59,   633,  2599,     4,     2],\n",
       "         [ 9497,    33,   703, 14363,  3156,    25,   233,     9,  4941,    88,\n",
       "             41,  2080,   751,    41,  1586,  7450,  2681,  2003,    94,    76,\n",
       "              4,     2,     1,     1,     1,     1,     1]])}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoader(object):\n",
    "    def __init__(self, fairseq_dict, source_path, target_path, max_positions=1024, no_bos=True, pad_idx=1, eos_idx=2):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            fairseq_dict (fairseq.data.dictionary.Dictionary): fairseq dictionary.\n",
    "            source_path (str): path to bpe encoded source.\n",
    "            target_path (str): path to bpe encoded target.\n",
    "            max_positions (int): max sentence length.\n",
    "            no_bos (bool): whether append bos token.\n",
    "\n",
    "        \"\"\"\n",
    "        self.fairseq_dict = fairseq_dict\n",
    "        self.source_path = source_path\n",
    "        self.target_path = target_path\n",
    "\n",
    "        self.max_positions = max_positions\n",
    "        self.no_bos = no_bos\n",
    "        self.pad_idx = pad_idx\n",
    "        self.eos_idx = eos_idx\n",
    "\n",
    "        source = DataLoader.read_lines(source_path)\n",
    "        target = DataLoader.read_lines(target_path)\n",
    "        assert len(source) == len(target), \"Source and target size do NOT match!\"\n",
    "\n",
    "        self.data = self.build_sample(source, target)\n",
    "        del source, target\n",
    "\n",
    "    def build_sample(self, source, target):\n",
    "        data = []\n",
    "        for i, (s, t) in enumerate(zip(source, target)):\n",
    "            data.append({\n",
    "                'id': i,\n",
    "                'source': self.bpe_to_ids(s),\n",
    "                'target': self.bpe_to_ids(t)\n",
    "            })\n",
    "        return data\n",
    "\n",
    "    @staticmethod\n",
    "    def read_lines(file_path):\n",
    "        files = []\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                files.append(line.strip())\n",
    "        return files\n",
    "\n",
    "    def batch_iter(self, batch_size):\n",
    "        \"\"\"Create a batch of data.\n",
    "        \"\"\"\n",
    "        samples = []\n",
    "        for d in self.data:\n",
    "            if len(samples) == batch_size:\n",
    "                yield collate(samples, self.pad_idx, self.eos_idx, \n",
    "                              left_pad_source=True,\n",
    "                              left_pad_target=False,\n",
    "                              input_feeding=True)\n",
    "                samples = []\n",
    "\n",
    "            samples += [d]\n",
    "\n",
    "        if len(samples) != 0:\n",
    "            yield collate(samples, self.pad_idx, self.eos_idx,\n",
    "                          left_pad_source=True,\n",
    "                          left_pad_target=False,\n",
    "                          input_feeding=True)\n",
    "\n",
    "    def bpe_to_ids(self, sentence, addl_sentence=None):\n",
    "        \"\"\"Convert bpe ids to model input ids.\n",
    "\n",
    "        Args:\n",
    "            sentence (str): bpe encoded sentence.\n",
    "            addl_sentence (str): bpe encoded sentence.\n",
    "\n",
    "        \"\"\"\n",
    "        extra_tokens = 2\n",
    "        bos, eos = '<s> ', ' </s>'\n",
    "\n",
    "        if self.no_bos:\n",
    "            bos = ''\n",
    "            extra_tokens = 1\n",
    "\n",
    "        if addl_sentence:\n",
    "            tokens = sentence + eos + ' ' + addl_sentence\n",
    "        else:\n",
    "            tokens = sentence\n",
    "\n",
    "        if len(tokens.split(' ')) > self.max_positions - extra_tokens:\n",
    "            tokens = ' '.join(tokens.split(' ')[:self.max_positions - extra_tokens])\n",
    "        bpe_sentence = bos + tokens + eos\n",
    "\n",
    "        tokens = self.fairseq_dict.encode_line(bpe_sentence, append_eos=False)\n",
    "        return tokens.long()\n",
    "\n",
    "    def shuffle(self):\n",
    "        random.shuffle(self.data)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __iter__(self):\n",
    "        return iter(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "val = DataLoader(src_dict, document_bpe_path, target_bpe_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "val.shuffle()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "for b in val.batch_iter(24):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_batch = next(val.batch_iter(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': tensor([11131,  2688,   793,  2162]),\n",
       " 'nsentences': 4,\n",
       " 'ntokens': 114,\n",
       " 'net_input': {'src_tokens': tensor([[ 1708,   172,  2206,  ...,    98,  3571,     2],\n",
       "          [18801, 20083,    16,  ...,   274,  1630,     2],\n",
       "          [    1,     1,     1,  ...,   599,     4,     2],\n",
       "          [    1,     1,     1,  ...,  5751,     4,     2]]),\n",
       "  'src_lengths': tensor([1024, 1024,  468,  191]),\n",
       "  'prev_output_tokens': tensor([[    2,   771,  4575,    16,    45,     5,    78,   631,    14,   606,\n",
       "               7,  1508,    77,  1686,    59,     5,    70,    12,  4310,     6,\n",
       "            3228,    12,  4416,  9556,  2422,  2690,     9,     5,  2762,     4],\n",
       "          [    2, 31230,   241,  1554,   257,  9688,   718, 18879,  2886,     7,\n",
       "               5,  1156,   526,    25,  6375,   471,   704, 10125, 15038,   817,\n",
       "             237,  1022,    13,     5,  5310,  3076,   177,   136,  5295,     4],\n",
       "          [    2, 29111,   415,  1728, 10394,   922,  4324,  1008,    70,     9,\n",
       "            8983,    18,   332,     7,  1451, 22963,     6,     8,  3014, 11875,\n",
       "            1991,     9,  4087,    13,    10,   796,  3666,   968,  1514,     4],\n",
       "          [    2,   250,  9768,  9577, 16293,  1295,  9694,   334, 19347,  4143,\n",
       "           14472,    14,    21,  3579,    31,    69,  2270,   334,    11, 15693,\n",
       "              34,    57,   303,     4,     1,     1,     1,     1,     1,     1]])},\n",
       " 'target': tensor([[  771,  4575,    16,    45,     5,    78,   631,    14,   606,     7,\n",
       "           1508,    77,  1686,    59,     5,    70,    12,  4310,     6,  3228,\n",
       "             12,  4416,  9556,  2422,  2690,     9,     5,  2762,     4,     2],\n",
       "         [31230,   241,  1554,   257,  9688,   718, 18879,  2886,     7,     5,\n",
       "           1156,   526,    25,  6375,   471,   704, 10125, 15038,   817,   237,\n",
       "           1022,    13,     5,  5310,  3076,   177,   136,  5295,     4,     2],\n",
       "         [29111,   415,  1728, 10394,   922,  4324,  1008,    70,     9,  8983,\n",
       "             18,   332,     7,  1451, 22963,     6,     8,  3014, 11875,  1991,\n",
       "              9,  4087,    13,    10,   796,  3666,   968,  1514,     4,     2],\n",
       "         [  250,  9768,  9577, 16293,  1295,  9694,   334, 19347,  4143, 14472,\n",
       "             14,    21,  3579,    31,    69,  2270,   334,    11, 15693,    34,\n",
       "             57,   303,     4,     2,     1,     1,     1,     1,     1,     1]])}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fairseq.utils import move_to_cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_batch = move_to_cuda(test_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_output = bart.model(**test_batch['net_input'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 30, 50265])\n"
     ]
    }
   ],
   "source": [
    "print(net_output[0].shape) # [batch_size, target_len, vocab_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['attn', 'inner_states'])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net_output[1].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "lprobs = bart.model.get_normalized_probs(net_output, log_probs=True)\n",
    "lprobs = lprobs.view(-1, lprobs.size(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([120, 50265])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lprobs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = test_batch[\"target\"].view(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([120, 1])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_smoothed_nll_loss(lprobs, target, epsilon, ignore_index=None, reduce=True):\n",
    "    if target.dim() == lprobs.dim() - 1:\n",
    "        target = target.unsqueeze(-1)\n",
    "    nll_loss = -lprobs.gather(dim=-1, index=target)\n",
    "    smooth_loss = -lprobs.sum(dim=-1, keepdim=True)\n",
    "    if ignore_index is not None:\n",
    "        pad_mask = target.eq(ignore_index)\n",
    "        nll_loss.masked_fill_(pad_mask, 0.)\n",
    "        smooth_loss.masked_fill_(pad_mask, 0.)\n",
    "    else:\n",
    "        nll_loss = nll_loss.squeeze(-1)\n",
    "        smooth_loss = smooth_loss.squeeze(-1)\n",
    "    if reduce:\n",
    "        nll_loss = nll_loss.sum()\n",
    "        smooth_loss = smooth_loss.sum()\n",
    "    eps_i = epsilon / lprobs.size(-1)\n",
    "    loss = (1. - epsilon) * nll_loss + eps_i * smooth_loss\n",
    "    return loss, nll_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "padding_idx = 1\n",
    "eps = 0.1\n",
    "loss, nll_loss = label_smoothed_nll_loss(\n",
    "    lprobs, target, eps, ignore_index=padding_idx, reduce=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(929.9940, device='cuda:0', grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args:\n",
    "    def __init__(self):\n",
    "        # scheduler\n",
    "        self.warmup_updates = 500\n",
    "        self.end_learning_rate = 0.00\n",
    "        self.total_num_update = 20000\n",
    "        self.power = 1.0\n",
    "        self.force_anneal = None\n",
    "        \n",
    "        # optimizer\n",
    "        self.lr = [3e-5]\n",
    "        self.adam_betas = '(0.9, 0.999)'\n",
    "        self.adam_eps = 1e-8\n",
    "        self.weight_decay = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Args()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fairseq.optim.adam import Adam, FairseqAdam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = list(\n",
    "            filter(\n",
    "                lambda p: p.requires_grad,\n",
    "                bart.model.parameters(),\n",
    "            )\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "adam = FairseqAdam(args, params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fairseq.optim.lr_scheduler.polynomial_decay_schedule import PolynomialDecaySchedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler = PolynomialDecaySchedule(args, adam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scheduler.step_update(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate Gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "adam.backward(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad_norm = adam.clip_grad_norm(0.1, aggregate_norm_fn=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Try BERT encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  133,   382,  1112,    34,  3946,   708,     7, 16888, 50264,  1751,\n",
       "         5656,     6,   217,     5, 20627,     9,  2398,   647,     7,    82,\n",
       "           15,  4952,  1183,  8204,     4], dtype=torch.int32)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_dict.encode_line('464 1294 3845 468 8606 3352 284 31833 <mask> 2485 6973 11 1390 262 17504 286 3777 4200 284 661 319 8649 2342 8341 13', append_eos=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The US Senate has rejected plans to tighten<unk> gun controls, including the restriction of weapons sales to people on terrorism watch lists.'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode_func(torch.tensor([  133,   382,  1112,    34,  3946,   708,     7, 16888,     3,  1751,\n",
    "          5656,     6,   217,     5, 20627,     9,  2398,   647,     7,    82,\n",
    "            15,  4952,  1183,  8204,     4,     2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The US Senate has rejected plans to tighten<mask> gun controls, including the restriction of weapons sales to people on terrorism watch lists.'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode_func(torch.tensor([ 133,   382,  1112,    34,  3946,   708,     7, 16888, 50264,  1751,\n",
    "         5656,     6,   217,     5, 20627,     9,  2398,   647,     7,    82,\n",
    "           15,  4952,  1183,  8204,     4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fairseq.models.bart.model.BARTModel"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(bart.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_func(target, src_tokens, src_lengths, prev_output_tokens):\n",
    "    print(src_tokens.shape)\n",
    "    print(src_lengths.shape)\n",
    "    print(prev_output_tokens.shape)\n",
    "    print(target.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 1024])\n",
      "torch.Size([4])\n",
      "torch.Size([4, 30])\n",
      "torch.Size([4, 30])\n"
     ]
    }
   ],
   "source": [
    "test_func(test_batch['target'], **test_batch['net_input'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fairseq.tasks.translation import TranslationTask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
